{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.117  Python-3.9.16 torch-1.8.0 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "Setup complete  (24 CPUs, 31.9 GB RAM, 616.9/903.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# yolo v8 체크\n",
    "import ultralytics\n",
    "\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4211980399169004211\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5762973696\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10849621960162805192\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# GPU 디바이스 네임 체크\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 80\n",
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# yolo 최초 모델 설정\n",
    "from ultralytics import YOLO as yolo\n",
    "\n",
    "ultralytics_model = yolo('yolov8n.pt')\n",
    "\n",
    "print(type(ultralytics_model.names), len(ultralytics_model.names))\n",
    "print(ultralytics_model.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Ti\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# GPU 활용 가능 여부 확인\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.119 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.117  Python-3.9.16 torch-1.8.0 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/HD_data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=None, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train35\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11135987 parameters, 11135971 gradients\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train35', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\HighDefinition_data\\labels\\train.cache... 4557 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4557/4557 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\HighDefinition_data\\labels\\val.cache... 1140 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1140/1140 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train35\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train35\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      3.87G     0.8747     0.7167     0.8782        105        640: 100%|██████████| 285/285 [03:43<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:52<00:00,  1.47s/it]\n",
      "                   all       1140       3248      0.987       0.97      0.992      0.867\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      3.76G     0.8322     0.4808     0.8639        117        640: 100%|██████████| 285/285 [03:38<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:49<00:00,  1.39s/it]\n",
      "                   all       1140       3248      0.973      0.977      0.983      0.876\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10       3.8G      0.804     0.4663     0.8633        124        640: 100%|██████████| 285/285 [03:33<00:00,  1.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:49<00:00,  1.38s/it]\n",
      "                   all       1140       3248      0.977      0.973      0.992      0.886\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      3.83G     0.7729     0.4381     0.8583        105        640: 100%|██████████| 285/285 [03:41<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:52<00:00,  1.47s/it]\n",
      "                   all       1140       3248      0.969      0.976      0.986      0.874\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10       3.8G      0.766     0.4281     0.8588        102        640: 100%|██████████| 285/285 [03:37<00:00,  1.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:53<00:00,  1.49s/it]\n",
      "                   all       1140       3248      0.991      0.969      0.993      0.904\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      3.81G     0.7498     0.4077     0.8533         93        640: 100%|██████████| 285/285 [03:41<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:52<00:00,  1.45s/it]\n",
      "                   all       1140       3248      0.973      0.976      0.992      0.903\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10       3.8G     0.7174     0.3917     0.8476        126        640: 100%|██████████| 285/285 [03:42<00:00,  1.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:51<00:00,  1.44s/it]\n",
      "                   all       1140       3248      0.972      0.982      0.992       0.91\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10       3.8G     0.7072     0.3785     0.8466         89        640: 100%|██████████| 285/285 [03:43<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:53<00:00,  1.48s/it]\n",
      "                   all       1140       3248       0.99       0.97      0.993      0.915\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      3.84G      0.686     0.3623     0.8416        112        640: 100%|██████████| 285/285 [03:40<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:50<00:00,  1.40s/it]\n",
      "                   all       1140       3248      0.975       0.98      0.992      0.915\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10       3.8G      0.669     0.3476     0.8395        120        640: 100%|██████████| 285/285 [03:31<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:53<00:00,  1.48s/it]\n",
      "                   all       1140       3248      0.976       0.98      0.992      0.919\n",
      "\n",
      "10 epochs completed in 0.756 hours.\n",
      "Optimizer stripped from runs\\detect\\train35\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train35\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train35\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.117  Python-3.9.16 torch-1.8.0 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 36/36 [00:51<00:00,  1.44s/it]\n",
      "                   all       1140       3248      0.976       0.98      0.992      0.919\n",
      "Speed: 0.1ms preprocess, 2.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train35\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 고해상도 데이터 학습\n",
    "\n",
    "ultralytics_model.train(data='C:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/HD_data.yaml', epochs=10, verbose=True, workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.119 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.117  Python-3.9.16 torch-1.8.0 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/User/Desktop/InsectData/ConvertData/Cam2_data/cam2_data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=None, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train36\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11135987 parameters, 11135971 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train36', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\labels\\train.cache... 1185 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1185/1185 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_36_58.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_36_59.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_37_04.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_49_20.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_49_21.jpg: 28 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_50_18.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_50_19.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_50_20.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_51_16.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_51_18.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_00_51_19.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_01_31_47.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_01_31_49.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_01_31_51.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_01_31_53.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_01_59_59.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_13.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_14.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_15.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_32.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_33.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_47.jpg: 28 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_03_48.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_30_22.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_50_58.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_50_59.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_02_51_00.jpg: 31 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_13_23.jpg: 31 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_13_24.jpg: 31 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_13_25.jpg: 31 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_14_01.jpg: 32 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_14_49.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_14_50.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_14_51.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_29_18.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_29_21.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_29_36.jpg: 21 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_29_38.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_29_59.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_30_01.jpg: 29 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_49_34.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_54_30.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\images\\train\\Capture_20211111_03_54_33.jpg: 30 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\Desktop\\InsectData\\ConvertData\\Cam2_data\\labels\\val.cache... 300 images, 0 backgrounds, 0 corrupt: 100%|██████████| 300/300 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train36\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train36\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      4.72G      1.736      1.124     0.8419         36        640: 100%|██████████| 75/75 [00:59<00:00,  1.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.17s/it]\n",
      "                   all        300       4508      0.646       0.48       0.53      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      4.32G      1.618     0.7008     0.8128         31        640: 100%|██████████| 75/75 [01:02<00:00,  1.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]\n",
      "                   all        300       4508      0.679      0.541      0.583      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      5.38G      1.609     0.6945       0.81         43        640: 100%|██████████| 75/75 [01:02<00:00,  1.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n",
      "                   all        300       4508      0.656      0.471      0.527      0.179\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      4.15G      1.534     0.6518     0.8075         18        640: 100%|██████████| 75/75 [01:01<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "                   all        300       4508      0.702      0.553      0.621      0.248\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      4.31G      1.482      0.631     0.8017         38        640: 100%|██████████| 75/75 [00:59<00:00,  1.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "                   all        300       4508      0.646      0.556      0.617      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      4.42G      1.492     0.6346     0.8024         27        640: 100%|██████████| 75/75 [00:57<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "                   all        300       4508       0.73      0.535      0.607      0.265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      4.89G      1.462     0.6193     0.8044         22        640: 100%|██████████| 75/75 [00:59<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\n",
      "                   all        300       4508      0.723      0.566      0.648      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      4.61G      1.452     0.6078     0.8003         64        640: 100%|██████████| 75/75 [01:00<00:00,  1.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.20s/it]\n",
      "                   all        300       4508       0.73       0.56      0.649      0.293\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      4.61G      1.379     0.5926     0.8002         12        640: 100%|██████████| 75/75 [01:04<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:11<00:00,  1.17s/it]\n",
      "                   all        300       4508      0.725      0.556       0.64      0.285\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      4.38G      1.347     0.5711     0.7997         15        640: 100%|██████████| 75/75 [01:01<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:12<00:00,  1.24s/it]\n",
      "                   all        300       4508      0.712      0.567      0.649      0.287\n",
      "\n",
      "10 epochs completed in 0.203 hours.\n",
      "Optimizer stripped from runs\\detect\\train36\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train36\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train36\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.117  Python-3.9.16 torch-1.8.0 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "                   all        300       4508       0.73       0.56       0.65      0.293\n",
      "Speed: 0.1ms preprocess, 3.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train36\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 2번 카메라 데이터 학습\n",
    "# model = yolo('C:/Users/User/Desktop/InsectData/runs/detect/고해상도데이터best.pt/weights/best.pt')\n",
    "ultralytics_model.train(data='C:/Users/User/Desktop/InsectData/ConvertData/Cam2_data/cam2_data.yaml', workers=0, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\User\\Desktop\\InsectData\\Data\\Training\\rawData\\TS1\\PtecticusTenebrifer\\Caterpillar\\HighDefinition\\Capture_20211115_15_27_37.jpg: 384x640 20 bugs, 19.0ms\n",
      "Speed: 1.0ms preprocess, 19.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "result = ultralytics_model.predict(source=\"C:/Users/User/Desktop/InsectData/Data/Training/rawData/TS1/PtecticusTenebrifer/Caterpillar/HighDefinition/Capture_20211115_15_27_37.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m416\u001b[39m, \u001b[39m416\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m best \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/User/Desktop/InsectData/runs/detect/train36/weights/best.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(best, dummy_input, \u001b[39m\"\u001b[39;49m\u001b[39mmodel.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\site-packages\\torch\\onnx\\__init__.py:271\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mExport a model into ONNX format.  This exporter runs your model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39monce in order to get a trace of its execution to be exported;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m        than ONNX.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m--> 271\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mexport(model, args, f, export_params, verbose, training,\n\u001b[0;32m    272\u001b[0m                     input_names, output_names, aten, export_raw_ir,\n\u001b[0;32m    273\u001b[0m                     operator_export_type, opset_version, _retain_param_name,\n\u001b[0;32m    274\u001b[0m                     do_constant_folding, example_outputs,\n\u001b[0;32m    275\u001b[0m                     strip_doc_string, dynamic_axes, keep_initializers_as_inputs,\n\u001b[0;32m    276\u001b[0m                     custom_opsets, enable_onnx_checker, use_external_data_format)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\site-packages\\torch\\onnx\\utils.py:88\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         operator_export_type \u001b[39m=\u001b[39m OperatorExportTypes\u001b[39m.\u001b[39mONNX\n\u001b[1;32m---> 88\u001b[0m _export(model, args, f, export_params, verbose, training, input_names, output_names,\n\u001b[0;32m     89\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type, opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m     90\u001b[0m         _retain_param_name\u001b[39m=\u001b[39;49m_retain_param_name, do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m     91\u001b[0m         example_outputs\u001b[39m=\u001b[39;49mexample_outputs, strip_doc_string\u001b[39m=\u001b[39;49mstrip_doc_string,\n\u001b[0;32m     92\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes, keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m     93\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets, enable_onnx_checker\u001b[39m=\u001b[39;49menable_onnx_checker,\n\u001b[0;32m     94\u001b[0m         use_external_data_format\u001b[39m=\u001b[39;49muse_external_data_format)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\site-packages\\torch\\onnx\\utils.py:676\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference, use_new_jit_passes)\u001b[0m\n\u001b[0;32m    674\u001b[0m _set_opset_version(opset_version)\n\u001b[0;32m    675\u001b[0m _set_operator_export_type(operator_export_type)\n\u001b[1;32m--> 676\u001b[0m \u001b[39mwith\u001b[39;00m select_model_mode_for_export(model, training):\n\u001b[0;32m    677\u001b[0m     val_keep_init_as_ip \u001b[39m=\u001b[39m _decide_keep_init_as_input(keep_initializers_as_inputs,\n\u001b[0;32m    678\u001b[0m                                                      operator_export_type,\n\u001b[0;32m    679\u001b[0m                                                      opset_version)\n\u001b[0;32m    680\u001b[0m     val_add_node_names \u001b[39m=\u001b[39m _decide_add_node_names(add_node_names, operator_export_type)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\site-packages\\torch\\onnx\\utils.py:38\u001b[0m, in \u001b[0;36mselect_model_mode_for_export\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_model_mode_for_export\u001b[39m(model, mode):\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction):\n\u001b[1;32m---> 38\u001b[0m         is_originally_training \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtraining\n\u001b[0;32m     40\u001b[0m         \u001b[39mif\u001b[39;00m mode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m             mode \u001b[39m=\u001b[39m TrainingMode\u001b[39m.\u001b[39mEVAL\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'training'"
     ]
    }
   ],
   "source": [
    "# model = ultralytics_model.model\n",
    "# model = model.cuda()\n",
    "dummy_input = torch.randn(1, 3, 416, 416, device='cuda')\n",
    "best = \"C:/Users/User/Desktop/InsectData/runs/detect/train36/weights/best.pt\"\n",
    "torch.onnx.export(best, dummy_input, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11.0ms\n",
      "c:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif self.dynamic or self.shape != shape:\n",
      "c:\\Users\\User\\anaconda3\\envs\\Dbug\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:346: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator. \n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     15\u001b[0m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mexport(model, dummy_input, \u001b[39m\"\u001b[39m\u001b[39mmodel.onnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 학습된 가중치 로드\n",
    "# weights = 'C:/Users/User/Desktop/InsectData/runs/detect/train36/weights/best.pt'\n",
    "# chkpt = torch.load(weights, map_location=torch.device('cpu'))\n",
    "# model = ultralytics_model(cfg=chkpt['model'].yaml, ch=3, nc=chkpt['model'].nc)  # nc is the number of classes\n",
    "# model.load_state_dict(chkpt['model'].float().state_dict())  # load model weights\n",
    "\n",
    "model = yolo('C:/Users/User/Desktop/InsectData/runs/detect/train36/weights/best.pt')\n",
    "\n",
    "# ONNX로 변환\n",
    "dummy_input = torch.randn(1, 3, 416, 416, device='cuda')\n",
    "output = model(dummy_input)\n",
    "model = model.model.cuda()\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input Layer:\n",
      "input.1: [1, 3, 416, 416]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# ONNX 모델 로드\n",
    "model = onnx.load('C:/Users/User/Desktop/InsectData/model.onnx')\n",
    "\n",
    "# 입력 레이어 출력\n",
    "print('Model Input Layer:')\n",
    "for input in model.graph.input:\n",
    "    print(input.name, end=': ')\n",
    "    # 레이어의 형상 출력\n",
    "    shape = [dim.dim_value for dim in input.type.tensor_type.shape.dim]\n",
    "    print(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.1\n",
      "420\n",
      "(1, 1, 5, 3549)\n",
      "Output node 0 shape: (1, 5, 3549)\n",
      "[[[     7.9263      19.316      30.172 ...      343.31      372.69      395.38]\n",
      "  [      5.292      4.8183      12.656 ...      372.76      376.34      395.04]\n",
      "  [      13.83      36.187      57.525 ...       152.6      144.99      114.26]\n",
      "  [     9.8277      10.043      26.687 ...      196.96      183.07      158.32]\n",
      "  [          0           0           0 ...  8.9407e-08  1.4901e-07  5.9605e-08]]]\n",
      "[2 0 1]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "classes = []\n",
    "\n",
    "# 클래스 라벨 불러오기\n",
    "with open(\"C:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/labels/classes.txt\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# ONNX 런타임 세션 생성\n",
    "sess = onnxruntime.InferenceSession(\"C:/Users/User/Desktop/InsectData/model.onnx\")\n",
    "\n",
    "# 이미지 불러오기\n",
    "image = cv2.imread(\"C:/Users/User/Desktop/InsectData/Data/Training/rawData/TS1/PtecticusTenebrifer/Caterpillar/HighDefinition/Capture_20211115_15_27_37.jpg\")\n",
    "image = cv2.resize(image, None, fx=0.4, fy=0.4)\n",
    "height, width, channels = image.shape\n",
    "colors = np.random.uniform(0, 255, 3)\n",
    "\n",
    "# 이미지 전처리\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "blob = blob.transpose((0, 1, 2, 3))  # ONNX는 CHW가 아니라 HWC 포맷을 기대합니다\n",
    "\n",
    "# 모델 실행 및 출력 얻기\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "outs = sess.run([output_name],{input_name: blob})\n",
    "\n",
    "print(input_name)\n",
    "print(output_name)\n",
    "print(np.array(outs).shape)\n",
    "\n",
    "for i, out in enumerate(outs):\n",
    "    print(f\"Output node {i} shape: {out.shape}\")\n",
    "    print(out)\n",
    "\n",
    "# 결과 처리\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "# 각 출력 노드에 대해 반복\n",
    "for out in outs:\n",
    "    # 첫 번째 차원(1)은 배치 크기를 나타내므로 우리는 이를 무시할 수 있습니다.\n",
    "    # 모든 그리드 셀에 대해 순회합니다.\n",
    "    for grid in out[0]:\n",
    "        # 바운딩 박스의 x, y 좌표, 너비, 높이, 그리고 객체 존재 확률을 추출합니다.\n",
    "        center_x, center_y, w, h, objectness = grid[:5]\n",
    "\n",
    "        # 객체 존재 확률이 0.5보다 크면, 해당 그리드 셀에 대한 예측을 저장합니다.\n",
    "        if objectness > 0.1:\n",
    "            # 이미지의 크기에 따라 바운딩 박스의 좌표와 크기를 스케일링합니다.\n",
    "            center_x = int(center_x * image.shape[1])\n",
    "            center_y = int(center_y * image.shape[0])\n",
    "            w = int(w * image.shape[1])\n",
    "            h = int(h * image.shape[0])\n",
    "\n",
    "            # 바운딩 박스의 좌표를 계산합니다 (YOLO는 바운딩 박스의 중심 좌표를 예측하므로, 우리는 이를 바운딩 박스의 좌측 상단 좌표로 변환해야 합니다).\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(objectness))\n",
    "\n",
    "            # 객체의 클래스 ID를 찾기 위해, 나머지 값을 확률로 간주하고 이 중에서 가장 높은 확률을 가진 클래스를 선택합니다.\n",
    "            # 나머지 값을 확률로 간주하고 이 중에서 가장 높은 확률을 가진 클래스를 선택합니다.\n",
    "            class_probabilities = grid[5:]\n",
    "            class_id = np.argmax(class_probabilities)\n",
    "            class_ids.append(class_id)\n",
    "            # classes 리스트의 길이를 초과하지 않는지 확인합니다.\n",
    "            # if class_id < len(classes):\n",
    "            #     class_ids.append(class_id)\n",
    "            # else:\n",
    "            #     print(f\"Warning: Predicted class id {class_id} is out of range of classes list with length {len(classes)}. Ignoring this box.\")\n",
    "\n",
    "\n",
    "# for out in outs:\n",
    "#     for detection in out:\n",
    "#         scores = detection[5:]\n",
    "#         class_id = np.argmax(scores)\n",
    "#         confidence = scores[class_id]\n",
    "#         if confidence > 0.5:\n",
    "#             # Object detected\n",
    "#             center_x = int(detection[0] * width)\n",
    "#             center_y = int(detection[1] * height)\n",
    "#             w = int(detection[2] * width)\n",
    "#             h = int(detection[3] * height)\n",
    "#             # 좌표\n",
    "#             x = int(center_x - w / 2)\n",
    "#             y = int(center_y - h / 2)\n",
    "#             boxes.append([x, y, w, h])\n",
    "#             confidences.append(float(confidence))\n",
    "#             class_ids.append(class_id)\n",
    "\n",
    "# Non-Maxima Suppression 사용하여 겹치는 박스 제거\n",
    "if boxes and confidences:  # 리스트가 비어 있지 않은지 확인\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    print(indexes)\n",
    "else:\n",
    "    indexes = []\n",
    "\n",
    "# 감지된 객체에 바운딩 박스와 레이블 그리기\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(class_ids[i])\n",
    "        color = colors[0]\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label, (x, y + 30), font, 3, color, 3)\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "onnx_model = \"C:/Users/User/Desktop/InsectData/model.onnx\"\n",
    "net = cv2.dnn.readNetFromONNX(onnx_model)\n",
    "\n",
    "image = cv2.imread(\"C:/Users/User/Desktop/InsectData/Data/Training/rawData/TS1/PtecticusTenebrifer/Caterpillar/Cam3/Capture_20211109_15_49_04.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\reshape_layer.cpp:150: error: (-1:Backtrace) Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m blob \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mblobFromImage(image, \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255.0\u001b[39m, (\u001b[39m416\u001b[39m, \u001b[39m416\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), swapRB\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, crop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m net\u001b[39m.\u001b[39msetInput(blob)\n\u001b[1;32m---> 22\u001b[0m outs \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mforward()\n\u001b[0;32m     24\u001b[0m class_ids \u001b[39m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m confidences \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\reshape_layer.cpp:150: error: (-1:Backtrace) Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ONNX 모델 로드\n",
    "net = cv2.dnn.readNetFromONNX(\"C:/Users/User/Desktop/InsectData/runs/detect/train31/weights/best.onnx\")\n",
    "\n",
    "classes = []\n",
    "with open(\"C:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/labels/classes.txt\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# 이미지 로드\n",
    "image = cv2.imread(\"C:/Users/User/Desktop/InsectData/Data/Training/rawData/TS1/PtecticusTenebrifer/Caterpillar/Cam3/Capture_20211109_15_49_04.jpg\")\n",
    "\n",
    "# 이미지 크기 조정\n",
    "image = cv2.resize(image, (640, 640))\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(image, 1.0/255.0, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "net.setInput(blob)\n",
    "outs = net.forward()\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        # 신뢰도 임계값 설정 (예: 0.5)\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * image.shape[1])\n",
    "            center_y = int(detection[1] * image.shape[0])\n",
    "            w = int(detection[2] * image.shape[1])\n",
    "            h = int(detection[3] * image.shape[0])\n",
    "\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label, (x, y + 30), font, 3, color, 3)\n",
    "\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:1073: error: (-2:Unspecified error) in function 'cv::dnn::dnn4_v20221220::ONNXImporter::handleNode'\n> Node [NonZero@ai.onnx]:(onnx_node!NonZero_223) parse error: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:375: error: (-215:Assertion failed) (bool)layer in function 'cv::dnn::dnn4_v20221220::runLayer'\n> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# ONNX 모델 로드\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m net \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromONNX(\u001b[39m\"\u001b[39;49m\u001b[39mC:/Users/User/Desktop/InsectData/model.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m classes \u001b[39m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mC:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/labels/classes.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:1073: error: (-2:Unspecified error) in function 'cv::dnn::dnn4_v20221220::ONNXImporter::handleNode'\n> Node [NonZero@ai.onnx]:(onnx_node!NonZero_223) parse error: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:375: error: (-215:Assertion failed) (bool)layer in function 'cv::dnn::dnn4_v20221220::runLayer'\n> "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ONNX 모델 로드\n",
    "net = cv2.dnn.readNetFromONNX(\"C:/Users/User/Desktop/InsectData/model.onnx\")\n",
    "\n",
    "classes = []\n",
    "with open(\"C:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/labels/classes.txt\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# 이미지 로드\n",
    "image = cv2.imread(\"C:/Users/User/Desktop/InsectData/Data/Training/rawData/TS1/PtecticusTenebrifer/Caterpillar/Cam3/Capture_20211109_15_49_04.jpg\")\n",
    "\n",
    "# 이미지 크기 조정\n",
    "image = cv2.resize(image, (640, 640))\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255, (640, 640), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "net.setInput(blob)\n",
    "outs = net.forward()\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        # 신뢰도 임계값 설정 (예: 0.5)\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * image.shape[1])\n",
    "            center_y = int(detection[1] * image.shape[0])\n",
    "            w = int(detection[2] * image.shape[1])\n",
    "            h = int(detection[3] * image.shape[0])\n",
    "\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label, (x, y + 30), font, 3, color, 3)\n",
    "\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "('onnx_node!Conv_0', 'onnx_node!Sigmoid_1', 'onnx_node!Mul_2', 'onnx_node!Conv_3', 'onnx_node!Sigmoid_4', 'onnx_node!Mul_5', 'onnx_node!Conv_6', 'onnx_node!Sigmoid_7', 'onnx_node!Mul_8', 'onnx_node!Split_9', 'onnx_node!Conv_10', 'onnx_node!Sigmoid_11', 'onnx_node!Mul_12', 'onnx_node!Conv_13', 'onnx_node!Sigmoid_14', 'onnx_node!Mul_15', 'onnx_node!Add_16', 'onnx_node!Concat_17', 'onnx_node!Conv_18', 'onnx_node!Sigmoid_19', 'onnx_node!Mul_20', 'onnx_node!Conv_21', 'onnx_node!Sigmoid_22', 'onnx_node!Mul_23', 'onnx_node!Conv_24', 'onnx_node!Sigmoid_25', 'onnx_node!Mul_26', 'onnx_node!Split_27', 'onnx_node!Conv_28', 'onnx_node!Sigmoid_29', 'onnx_node!Mul_30', 'onnx_node!Conv_31', 'onnx_node!Sigmoid_32', 'onnx_node!Mul_33', 'onnx_node!Add_34', 'onnx_node!Conv_35', 'onnx_node!Sigmoid_36', 'onnx_node!Mul_37', 'onnx_node!Conv_38', 'onnx_node!Sigmoid_39', 'onnx_node!Mul_40', 'onnx_node!Add_41', 'onnx_node!Concat_42', 'onnx_node!Conv_43', 'onnx_node!Sigmoid_44', 'onnx_node!Mul_45', 'onnx_node!Conv_46', 'onnx_node!Sigmoid_47', 'onnx_node!Mul_48', 'onnx_node!Conv_49', 'onnx_node!Sigmoid_50', 'onnx_node!Mul_51', 'onnx_node!Split_52', 'onnx_node!Conv_53', 'onnx_node!Sigmoid_54', 'onnx_node!Mul_55', 'onnx_node!Conv_56', 'onnx_node!Sigmoid_57', 'onnx_node!Mul_58', 'onnx_node!Add_59', 'onnx_node!Conv_60', 'onnx_node!Sigmoid_61', 'onnx_node!Mul_62', 'onnx_node!Conv_63', 'onnx_node!Sigmoid_64', 'onnx_node!Mul_65', 'onnx_node!Add_66', 'onnx_node!Concat_67', 'onnx_node!Conv_68', 'onnx_node!Sigmoid_69', 'onnx_node!Mul_70', 'onnx_node!Conv_71', 'onnx_node!Sigmoid_72', 'onnx_node!Mul_73', 'onnx_node!Conv_74', 'onnx_node!Sigmoid_75', 'onnx_node!Mul_76', 'onnx_node!Split_77', 'onnx_node!Conv_78', 'onnx_node!Sigmoid_79', 'onnx_node!Mul_80', 'onnx_node!Conv_81', 'onnx_node!Sigmoid_82', 'onnx_node!Mul_83', 'onnx_node!Add_84', 'onnx_node!Concat_85', 'onnx_node!Conv_86', 'onnx_node!Sigmoid_87', 'onnx_node!Mul_88', 'onnx_node!Conv_89', 'onnx_node!Sigmoid_90', 'onnx_node!Mul_91', 'onnx_node!MaxPool_92', 'onnx_node!MaxPool_93', 'onnx_node!MaxPool_94', 'onnx_node!Concat_95', 'onnx_node!Conv_96', 'onnx_node!Sigmoid_97', 'onnx_node!Mul_98', 'onnx_node!Resize_100', 'onnx_node!Concat_101', 'onnx_node!Conv_102', 'onnx_node!Sigmoid_103', 'onnx_node!Mul_104', 'onnx_node!Split_105', 'onnx_node!Conv_106', 'onnx_node!Sigmoid_107', 'onnx_node!Mul_108', 'onnx_node!Conv_109', 'onnx_node!Sigmoid_110', 'onnx_node!Mul_111', 'onnx_node!Concat_112', 'onnx_node!Conv_113', 'onnx_node!Sigmoid_114', 'onnx_node!Mul_115', 'onnx_node!Resize_117', 'onnx_node!Concat_118', 'onnx_node!Conv_119', 'onnx_node!Sigmoid_120', 'onnx_node!Mul_121', 'onnx_node!Split_122', 'onnx_node!Conv_123', 'onnx_node!Sigmoid_124', 'onnx_node!Mul_125', 'onnx_node!Conv_126', 'onnx_node!Sigmoid_127', 'onnx_node!Mul_128', 'onnx_node!Concat_129', 'onnx_node!Conv_130', 'onnx_node!Sigmoid_131', 'onnx_node!Mul_132', 'onnx_node!Conv_133', 'onnx_node!Sigmoid_134', 'onnx_node!Mul_135', 'onnx_node!Concat_136', 'onnx_node!Conv_137', 'onnx_node!Sigmoid_138', 'onnx_node!Mul_139', 'onnx_node!Split_140', 'onnx_node!Conv_141', 'onnx_node!Sigmoid_142', 'onnx_node!Mul_143', 'onnx_node!Conv_144', 'onnx_node!Sigmoid_145', 'onnx_node!Mul_146', 'onnx_node!Concat_147', 'onnx_node!Conv_148', 'onnx_node!Sigmoid_149', 'onnx_node!Mul_150', 'onnx_node!Conv_151', 'onnx_node!Sigmoid_152', 'onnx_node!Mul_153', 'onnx_node!Concat_154', 'onnx_node!Conv_155', 'onnx_node!Sigmoid_156', 'onnx_node!Mul_157', 'onnx_node!Split_158', 'onnx_node!Conv_159', 'onnx_node!Sigmoid_160', 'onnx_node!Mul_161', 'onnx_node!Conv_162', 'onnx_node!Sigmoid_163', 'onnx_node!Mul_164', 'onnx_node!Concat_165', 'onnx_node!Conv_166', 'onnx_node!Sigmoid_167', 'onnx_node!Mul_168', 'onnx_node!Conv_172', 'onnx_node!Sigmoid_173', 'onnx_node!Mul_174', 'onnx_node!Conv_175', 'onnx_node!Sigmoid_176', 'onnx_node!Mul_177', 'onnx_node!Conv_178', 'onnx_node!Conv_179', 'onnx_node!Sigmoid_180', 'onnx_node!Mul_181', 'onnx_node!Conv_182', 'onnx_node!Sigmoid_183', 'onnx_node!Mul_184', 'onnx_node!Conv_185', 'onnx_node!Concat_186', 'onnx_node!Conv_187', 'onnx_node!Sigmoid_188', 'onnx_node!Mul_189', 'onnx_node!Conv_190', 'onnx_node!Sigmoid_191', 'onnx_node!Mul_192', 'onnx_node!Conv_193', 'onnx_node!Conv_194', 'onnx_node!Sigmoid_195', 'onnx_node!Mul_196', 'onnx_node!Conv_197', 'onnx_node!Sigmoid_198', 'onnx_node!Mul_199', 'onnx_node!Conv_200', 'onnx_node!Concat_201', 'onnx_node!Conv_202', 'onnx_node!Sigmoid_203', 'onnx_node!Mul_204', 'onnx_node!Conv_205', 'onnx_node!Sigmoid_206', 'onnx_node!Mul_207', 'onnx_node!Conv_208', 'onnx_node!Conv_209', 'onnx_node!Sigmoid_210', 'onnx_node!Mul_211', 'onnx_node!Conv_212', 'onnx_node!Sigmoid_213', 'onnx_node!Mul_214', 'onnx_node!Conv_215', 'onnx_node!Concat_216', 'onnx_node!Reshape_219', 'onnx_node!Reshape_222', 'onnx_node!Reshape_225', 'onnx_node!Concat_226', 'onnx_node!Split_227', 'onnx_node!Reshape_237', 'onnx_node!Transpose_238', 'onnx_node!Softmax_239', 'onnx_node!Transpose_240', 'onnx_node!Conv_241', 'onnx_node!Reshape_245', 'onnx_node!Slice_256', 'onnx_node!Slice_259', '421', 'onnx_node!Sub_261', '423', 'onnx_node!Add_263', 'onnx_node!Add_264', '446', 'onnx_node!Div_265', 'onnx_node!Sub_266', 'onnx_node!Concat_267', '431', 'onnx_node!Mul_269', 'onnx_node!Sigmoid_270', 'onnx_node!Concat_271', 'output0')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(layer_names))\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(layer_names)\n\u001b[1;32m---> 15\u001b[0m output_layers \u001b[39m=\u001b[39m [layer_names[i[\u001b[39m0\u001b[39m]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m     18\u001b[0m colors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39mlen\u001b[39m(classes), \u001b[39m3\u001b[39m))\n\u001b[0;32m     20\u001b[0m \u001b[39m# 이미지 로드 및 전처리\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(layer_names))\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(layer_names)\n\u001b[1;32m---> 15\u001b[0m output_layers \u001b[39m=\u001b[39m [layer_names[i[\u001b[39m0\u001b[39;49m]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m     18\u001b[0m colors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39mlen\u001b[39m(classes), \u001b[39m3\u001b[39m))\n\u001b[0;32m     20\u001b[0m \u001b[39m# 이미지 로드 및 전처리\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ONNX 모델 로드\n",
    "net = cv2.dnn.readNetFromONNX(\"C:/Users/User/Desktop/InsectData/model.onnx\")\n",
    "classes = []\n",
    "\n",
    "with open(\"C:/Users/User/Desktop/InsectData/ConvertData/HighDefinition_data/labels/classes.txt\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "print(type(layer_names))\n",
    "print(layer_names)\n",
    "\n",
    "output_layers = [layer_names[i[0]] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "image = cv2.imread(\"C:/Users/User/Desktop/InsectData/Data/Training/rawData/TS1/PtecticusTenebrifer/Caterpillar/Cam3/Capture_20211109_15_49_04.jpg\")\n",
    "image = cv2.resize(img, None, fx=0.4, fy=0.4)\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255, (640, 640), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "net.setInput(blob)\n",
    "# outs = net.forward(output_layers)\n",
    "\n",
    "# print(type(outs))\n",
    "# print(outs)\n",
    "# class_ids = []\n",
    "# confidences = []\n",
    "# boxes = []\n",
    "\n",
    "# for out in outs:\n",
    "#     for detection in out:\n",
    "#         # 감지 결과에서 bounding box와 클래스 ID, 신뢰도 추출\n",
    "#         scores = detection[5:]\n",
    "#         classID = np.argmax(scores)\n",
    "#         confidence = scores[classID]\n",
    "        \n",
    "#         # 신뢰도 임계값 설정 (예: 0.5)\n",
    "#         if confidence > 0.5:\n",
    "#             center_x = int(detection[0] * width)\n",
    "#             center_y = int(detection[1] * height)\n",
    "#             w = int(detection[2] * width)\n",
    "#             h = int(detection[3] * height)\n",
    "\n",
    "#             x = int(center_x - w / 2)\n",
    "#             y = int(center_y - h / 2)\n",
    "#             boxes.append([x, y, w, h])\n",
    "#             confidences.append(float(confidence))\n",
    "#             class_ids.append(classID)\n",
    "\n",
    "# indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.4, 0.4)\n",
    "# font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "# for i in range(len(boxes)):\n",
    "#     if i in indexes:\n",
    "#         w, y, w, h = boxes[i]\n",
    "#         class_name = classes[class_ids[i]]\n",
    "#         label = f\"{class_name} {confidences[i]:.2f}\"\n",
    "#         color = colors[class_ids[i]]\n",
    "\n",
    "#         cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "#         cv2.putText(image, label, (x, y + 30), font, 3, color, 3)\n",
    "\n",
    "# # 결과 이미지 표시\n",
    "# cv2.imshow(\"Image\", image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "with Image.open('C:/Users/User/Desktop/InsectData/runs/detect/predict/Capture_20211115_15_26_51.jpg') as pred_image:\n",
    "    pred_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dbug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
